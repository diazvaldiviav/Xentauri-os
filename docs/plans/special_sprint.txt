================================================================================
                    SPECIAL SPRINT - MEMORY PROJECTS MODULE
                   Context-Aware Content Generation Memory
================================================================================

Fecha de Creaci√≥n: 2024-12-27
Autor: Claude Sonnet 4.5 + Developer
Estado: READY FOR IMPLEMENTATION
Prerequisito: Sprint 4.1.0 Complete (Intelligent Conversations & Real-Time Data)

================================================================================
                              PROMPT PARA COPILOT
================================================================================

# PASO 1: ENTENDER EL PROYECTO COMPLETO

Antes de hacer cualquier cambio, necesitas analizar estos archivos para entender
la arquitectura completa del proyecto Jarvis:

1. Lee COMPLETAMENTE @CONTEXT.md
   - Entiende la arquitectura modular (app/ai/, app/services/, app/environments/)
   - Entiende el flujo de intents: Router ‚Üí Parser ‚Üí IntentService ‚Üí Handlers
   - Entiende UnifiedContext system (l√≠nea ~268 en app/ai/context.py)
   - Entiende los servicios existentes con TTL: pending_event_service, pending_edit_service, conversation_context_service
   - Entiende Scene Graph (Sprint 4.0): SceneService, ComponentRegistry, text_block component
   - Entiende el patr√≥n DRY: NO duplicar llamadas a LLMs, usar providers existentes

2. Lee COMPLETAMENTE @system.html
   - Entiende el estado actual del proyecto (Sprint 4.1.0 Complete)
   - Entiende los 679 tests pasando que NO deben romperse
   - Entiende los 15 servicios core y c√≥mo interact√∫an
   - Entiende la arquitectura de 3 capas: Router ‚Üí Intent ‚Üí Service

================================================================================
                              PROBLEMA A RESOLVER
================================================================================

S√çNTOMA EN LOS LOGS:
--------------------
Usuario: "Crea una nota ABA"
‚Üí Intent: CONVERSATION, genera contenido ‚úÖ
‚Üí Respuesta: "Aqu√≠ est√° tu nota ABA: [contenido generado]"

Usuario: "muestra la nota en la pantalla"
‚Üí Intent: DOC_QUERY ‚ùå (INCORRECTO - deber√≠a ser DISPLAY_CONTENT)
‚Üí Sistema no sabe que acabamos de generar una nota
‚Üí Busca en Google Docs, no encuentra nada
‚Üí Error: "No encontr√© ese documento"

PROBLEMA RA√çZ:
--------------
1. _handle_conversation() genera contenido pero NO lo registra en ning√∫n lado
2. conversation_context_service solo guarda pending_content_request, NO el contenido generado
3. Intent parser no tiene contexto de "acabamos de generar esto" para distinguir:
   - "muestra la nota" (contenido GENERADO) ‚Üí deber√≠a ser DISPLAY_CONTENT
   - "muestra el documento de Google" (contenido EXTERNO) ‚Üí deber√≠a ser DOC_QUERY
4. No hay bridge entre CONVERSATION intent y DISPLAY_CONTENT intent
5. Scene Graph existe y funciona, solo falta la fuente de datos de contenido generado

SOLUCI√ìN REQUERIDA:
-------------------
Crear un m√≥dulo Memory Projects que:
1. Registre contenido generado cuando _handle_conversation() lo crea
2. Mantenga memoria activa (in-memory con TTL) de proyectos generados
3. Inyecte contexto en UnifiedContext para que el LLM sepa "hay contenido generado pendiente"
4. Provea los datos para Scene Graph cuando usuario pida "muestra la nota"

================================================================================
                         AN√ÅLISIS DEL C√ìDIGO ACTUAL
================================================================================

ARCHIVOS CR√çTICOS QUE DEBES LEER ANTES DE MODIFICAR:
----------------------------------------------------

1. app/services/intent_service.py (l√≠neas 1405-1592)
   - M√©todo _handle_conversation(): Genera contenido con LLM pero NO lo guarda
   - L√≠neas 1551-1556: Solo guarda en conversation_history, no como displayable content
   - AQU√ç necesitas AGREGAR registro despu√©s de generar el mensaje

2. app/ai/context.py (l√≠neas 268-366, 372-525)
   - UnifiedContext dataclass: Necesitas AGREGAR campo memory_bridge
   - build_unified_context(): Necesitas AGREGAR inyecci√≥n del memory bridge
   - NO modificar campos existentes, solo AGREGAR nuevos

3. app/services/conversation_context_service.py (l√≠neas 44-109)
   - UserConversationState: Solo tiene pending_content_request (l√≠nea 77-79)
   - NO modificar esta clase, crear m√≥dulo separado para memoria
   - Este servicio maneja pending requests, el nuevo m√≥dulo maneja generated content

4. app/ai/prompts/intent_prompts.py (l√≠neas 145-219)
   - Reglas de clasificaci√≥n DOC_QUERY vs CONVERSATION
   - Necesitas AGREGAR reglas memory-aware DESPU√âS de las existentes
   - NO modificar reglas existentes, solo AGREGAR contexto din√°mico

5. app/ai/scene/service.py
   - SceneService.generate_scene() existe y funciona
   - SceneService.populate_scene_data() existe para llenar datos
   - ComponentRegistry tiene text_block component
   - NO modificar estos m√©todos, solo USAR en _handle_display_content()

6. app/ai/router/orchestrator.py (l√≠neas 164-165)
   - AIRouter inyecta context en prompts
   - Necesitas AGREGAR inyecci√≥n de memory_bridge.to_context_string()
   - NO modificar la l√≥gica de routing, solo AGREGAR contexto

================================================================================
                       DISE√ëO DE LA SOLUCI√ìN (DRY/SOLID)
================================================================================

PRINCIPIOS A SEGUIR:
--------------------
‚úÖ DRY (Don't Repeat Yourself):
   - NO duplicar llamadas a LLMs (usar providers existentes)
   - NO duplicar l√≥gica de TTL (seguir patr√≥n de pending_event_service)
   - NO duplicar l√≥gica de Scene Graph (usar SceneService existente)

‚úÖ SOLID:
   - Single Responsibility: Cada clase hace UNA cosa
   - Open/Closed: Extender sin modificar (agregar campos, no cambiar existentes)
   - Liskov Substitution: Nuevas clases compatibles con interfaces existentes
   - Interface Segregation: Interfaces peque√±as y espec√≠ficas
   - Dependency Inversion: Depender de abstracciones, no de implementaciones

‚úÖ Separation of Concerns:
   - app/ai/memory/ ‚Üí Schemas y l√≥gica de memoria (nueva)
   - app/services/ ‚Üí Service layer que usa la memoria (extender)
   - app/ai/prompts/ ‚Üí Prompts que incluyen contexto (extender)

================================================================================
                          ARCHIVOS A CREAR (NUEVOS)
================================================================================

M√ìDULO 1: app/ai/memory/__init__.py
------------------------------------
```python
"""
Memory Projects Module - Sprint 4.1.1

Tracks generated content across intent changes to enable display of AI-generated content.
Follows the same TTL pattern as pending_event_service and pending_edit_service.
"""

from app.ai.memory.schemas import (
    MemoryProject,
    ContentStatus,
    ContentType,
    MemoryBridge,
)
from app.ai.memory.service import memory_project_service

__all__ = [
    "MemoryProject",
    "ContentStatus",
    "ContentType",
    "MemoryBridge",
    "memory_project_service",
]
```

M√ìDULO 2: app/ai/memory/schemas.py
-----------------------------------
```python
"""
Memory Project Schemas

Dataclasses for tracking generated content lifecycle.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, List
from enum import Enum


class ContentStatus(Enum):
    """Content lifecycle status"""
    GENERATED = "generated"    # Just created, ready to display
    DISPLAYED = "displayed"    # Shown on screen
    ARCHIVED = "archived"      # Expired or manually archived


class ContentType(Enum):
    """Type of generated content"""
    NOTE = "note"
    EMAIL = "email"
    MESSAGE = "message"
    DOCUMENT = "document"
    SUMMARY = "summary"
    SCRIPT = "script"
    TEMPLATE = "template"


@dataclass
class MemoryProject:
    """
    Represents generated content that persists across intent changes.

    Similar to PendingEvent but for completed AI-generated content.
    Follows TTL pattern from pending_event_service (300s default).
    """
    project_id: str                          # UUID
    user_id: str
    content_type: ContentType
    content: str                             # Generated text content
    status: ContentStatus
    created_at: datetime
    last_accessed: datetime
    ttl_seconds: int = 300                   # 5 minutes (same as conversation_context)

    # Source tracking
    source_intent: str = "conversation"      # Which intent created this
    source_request: Optional[str] = None     # Original user request

    # Scene Graph metadata (for display)
    display_title: Optional[str] = None      # "Nota ABA", "Email Draft", etc.

    def is_expired(self) -> bool:
        """Check if project has exceeded TTL"""
        age = (datetime.now() - self.last_accessed).total_seconds()
        return age >= self.ttl_seconds


@dataclass
class MemoryBridge:
    """
    Context bridge injected into UnifiedContext.

    Provides memory-aware context to LLM for intent classification.
    """
    active_projects: List[MemoryProject] = field(default_factory=list)

    def has_recent_generated_content(self) -> bool:
        """Check if there's any generated content pending display"""
        return any(p.status == ContentStatus.GENERATED for p in self.active_projects)

    def get_latest_generated(self) -> Optional[MemoryProject]:
        """Get most recently generated project"""
        generated = [p for p in self.active_projects if p.status == ContentStatus.GENERATED]
        if not generated:
            return None
        return max(generated, key=lambda p: p.created_at)

    def to_context_string(self) -> str:
        """
        Format for injection into LLM prompts.

        This helps the LLM distinguish between:
        - "muestra la nota" (GENERATED content) ‚Üí DISPLAY_CONTENT
        - "muestra el documento de Google" (EXTERNAL content) ‚Üí DOC_QUERY
        """
        if not self.active_projects:
            return ""

        lines = ["ACTIVE MEMORY PROJECTS:"]
        for project in self.active_projects:
            status_emoji = "‚ö°" if project.status == ContentStatus.GENERATED else "‚úì"
            lines.append(
                f"{status_emoji} [{project.content_type.value}] {project.display_title or 'Untitled'}"
            )
            if project.status == ContentStatus.GENERATED:
                lines.append("  ‚ö†Ô∏è PENDING DISPLAY - User may reference this content")

        return "\n".join(lines)
```

M√ìDULO 3: app/ai/memory/service.py
-----------------------------------
```python
"""
Memory Project Service

In-memory storage for generated content with TTL cleanup.
Follows the same pattern as pending_event_service and conversation_context_service.
"""

import logging
from typing import Dict, Optional, List
from datetime import datetime
import uuid

from app.ai.memory.schemas import (
    MemoryProject,
    ContentStatus,
    ContentType,
    MemoryBridge,
)

logger = logging.getLogger("jarvis.ai.memory")


class MemoryProjectService:
    """
    Manages AI-generated content with TTL-based cleanup.

    Similar to:
    - pending_event_service: Stores data with TTL
    - conversation_context_service: Tracks user state

    Differences:
    - Stores COMPLETED content (not pending operations)
    - Bridges between CONVERSATION and DISPLAY_CONTENT intents
    """

    def __init__(self):
        # user_id -> List[MemoryProject]
        self._storage: Dict[str, List[MemoryProject]] = {}
        logger.info("MemoryProjectService initialized")

    def register_generated_content(
        self,
        user_id: str,
        content: str,
        content_type: str,
        source_request: str,
        display_title: Optional[str] = None,
        ttl_seconds: int = 300,
    ) -> MemoryProject:
        """
        Register content generated in _handle_conversation().

        This is the CRITICAL integration point that was missing.

        Args:
            user_id: User who generated the content
            content: The generated text
            content_type: "note", "email", "message", etc.
            source_request: Original user request ("crea una nota ABA")
            display_title: Optional title for display
            ttl_seconds: Time to live (default 300s = 5 min)

        Returns:
            MemoryProject: The registered project
        """
        project = MemoryProject(
            project_id=str(uuid.uuid4()),
            user_id=user_id,
            content_type=ContentType(content_type),
            content=content,
            status=ContentStatus.GENERATED,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            ttl_seconds=ttl_seconds,
            source_intent="conversation",
            source_request=source_request,
            display_title=display_title,
        )

        if user_id not in self._storage:
            self._storage[user_id] = []

        self._storage[user_id].append(project)
        self._cleanup_expired(user_id)

        logger.info(
            f"Registered memory project: {project.project_id} "
            f"({project.content_type.value}) for user {user_id}"
        )

        return project

    def mark_as_displayed(self, project_id: str, user_id: str) -> bool:
        """Mark project as displayed"""
        projects = self._storage.get(user_id, [])
        for project in projects:
            if project.project_id == project_id:
                project.status = ContentStatus.DISPLAYED
                project.last_accessed = datetime.now()
                logger.info(f"Marked project {project_id} as displayed")
                return True
        return False

    def get_memory_bridge(self, user_id: str) -> MemoryBridge:
        """
        Get memory bridge for UnifiedContext injection.

        This is injected into build_unified_context() and then into LLM prompts.
        Helps the LLM classify intents correctly.

        Args:
            user_id: User ID

        Returns:
            MemoryBridge: Context about active projects
        """
        self._cleanup_expired(user_id)
        projects = self._storage.get(user_id, [])

        # Only include non-archived projects
        active = [p for p in projects if p.status != ContentStatus.ARCHIVED]

        return MemoryBridge(active_projects=active)

    def _cleanup_expired(self, user_id: str):
        """Remove expired projects (TTL exceeded)"""
        if user_id not in self._storage:
            return

        active_projects = []
        archived_count = 0

        for project in self._storage[user_id]:
            if project.is_expired():
                archived_count += 1
            else:
                active_projects.append(project)

        self._storage[user_id] = active_projects

        if archived_count > 0:
            logger.info(f"Cleaned up {archived_count} expired projects for user {user_id}")

    def clear_user_memory(self, user_id: str):
        """Clear all memory for a user (for testing or manual reset)"""
        if user_id in self._storage:
            count = len(self._storage[user_id])
            del self._storage[user_id]
            logger.info(f"Cleared {count} projects for user {user_id}")


# Singleton instance (same pattern as pending_event_service)
memory_project_service = MemoryProjectService()
```

================================================================================
                    MODIFICACIONES A ARCHIVOS EXISTENTES
================================================================================

IMPORTANTE: NO modificar c√≥digo que funciona, solo AGREGAR nuevas funcionalidades.

================================================================================
                      FLUJO ENTRE MODELOS (CR√çTICO)
================================================================================

El contexto de memoria DEBE propagarse a trav√©s de TODOS los modelos:

FLUJO COMPLETO:
---------------

Request #1: "Crea una nota ABA"
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. build_unified_context(user_id, db)                                   ‚îÇ
‚îÇ    ‚îú‚îÄ conversation_context (multi-turn history)                         ‚îÇ
‚îÇ    ‚îú‚îÄ memory_bridge (VAC√çO - no hay proyectos)                          ‚îÇ
‚îÇ    ‚îî‚îÄ oauth_status, devices, etc.                                       ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ 2. AI Router (Gemini Flash)                                             ‚îÇ
‚îÇ    ‚îú‚îÄ Recibe: UnifiedContext con memory_bridge vac√≠o                    ‚îÇ
‚îÇ    ‚îú‚îÄ Clasifica: SIMPLE                                                 ‚îÇ
‚îÇ    ‚îî‚îÄ Ruta: Intent Parser                                               ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ 3. Intent Parser (Gemini Flash)                                         ‚îÇ
‚îÇ    ‚îú‚îÄ Recibe: UnifiedContext con memory_bridge vac√≠o                    ‚îÇ
‚îÇ    ‚îú‚îÄ Clasifica: CONVERSATION (detecta "crea")                          ‚îÇ
‚îÇ    ‚îî‚îÄ Intent: ConversationIntent                                        ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ 4. _handle_conversation (GPT-5.2 o Claude Opus 4.5)                     ‚îÇ
‚îÇ    ‚îú‚îÄ Genera contenido: "Aqu√≠ est√° tu nota ABA: [contenido]"            ‚îÇ
‚îÇ    ‚îú‚îÄ üÜï REGISTRA en memory_project_service:                            ‚îÇ
‚îÇ    ‚îÇ   ‚îú‚îÄ project_id: uuid                                              ‚îÇ
‚îÇ    ‚îÇ   ‚îú‚îÄ content: [contenido generado]                                 ‚îÇ
‚îÇ    ‚îÇ   ‚îú‚îÄ content_type: "note"                                          ‚îÇ
‚îÇ    ‚îÇ   ‚îú‚îÄ status: GENERATED                                             ‚îÇ
‚îÇ    ‚îÇ   ‚îî‚îÄ display_title: "Nota ABA"                                     ‚îÇ
‚îÇ    ‚îî‚îÄ Guarda en conversation_history (como antes)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Request #2: "muestra la nota en la pantalla"
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. build_unified_context(user_id, db)                                   ‚îÇ
‚îÇ    ‚îú‚îÄ conversation_context (incluye request anterior)                   ‚îÇ
‚îÇ    ‚îú‚îÄ üÜï memory_bridge (1 proyecto GENERATED):                          ‚îÇ
‚îÇ    ‚îÇ   ‚îî‚îÄ "‚ö° [note] Nota ABA - PENDING DISPLAY"                        ‚îÇ
‚îÇ    ‚îî‚îÄ oauth_status, devices, etc.                                       ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ 2. AI Router (Gemini Flash)                                             ‚îÇ
‚îÇ    ‚îú‚îÄ Recibe: UnifiedContext CON memory_bridge                          ‚îÇ
‚îÇ    ‚îú‚îÄ Ve en prompt: "ACTIVE MEMORY PROJECTS: ‚ö° [note] Nota ABA"        ‚îÇ
‚îÇ    ‚îú‚îÄ Clasifica: SIMPLE (sigue siendo simple)                           ‚îÇ
‚îÇ    ‚îî‚îÄ Ruta: Intent Parser                                               ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ 3. Intent Parser (Gemini Flash) ‚Üê CR√çTICO                               ‚îÇ
‚îÇ    ‚îú‚îÄ Recibe: UnifiedContext CON memory_bridge                          ‚îÇ
‚îÇ    ‚îú‚îÄ Ve en prompt: "ACTIVE MEMORY PROJECTS: ‚ö° [note] Nota ABA"        ‚îÇ
‚îÇ    ‚îú‚îÄ Ve reglas memory-aware:                                           ‚îÇ
‚îÇ    ‚îÇ   "If 'muestra la nota' AND [generated] note ‚Üí DISPLAY_CONTENT"    ‚îÇ
‚îÇ    ‚îú‚îÄ Clasifica: DISPLAY_CONTENT (NO DOC_QUERY) ‚úÖ                      ‚îÇ
‚îÇ    ‚îî‚îÄ Intent: DisplayContentIntent                                      ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ 4. _handle_display_content                                              ‚îÇ
‚îÇ    ‚îú‚îÄ Consulta memory_bridge.get_latest_generated()                     ‚îÇ
‚îÇ    ‚îú‚îÄ Encuentra: MemoryProject con contenido                            ‚îÇ
‚îÇ    ‚îú‚îÄ Usa Scene Graph (text_block component)                            ‚îÇ
‚îÇ    ‚îú‚îÄ Marca como DISPLAYED                                              ‚îÇ
‚îÇ    ‚îî‚îÄ Retorna SceneGraph con contenido embebido                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PUNTOS CR√çTICOS DE INYECCI√ìN:
------------------------------

1. app/ai/context.py - build_unified_context()
   ‚îî‚îÄ Crea memory_bridge ANTES de llamar a cualquier modelo

2. app/ai/router/orchestrator.py - analyze_request()
   ‚îî‚îÄ Inyecta memory_bridge.to_context_string() en prompt de Gemini Router

3. app/ai/intent/parser.py - parse()
   ‚îî‚îÄ Inyecta memory_bridge.to_context_string() en prompt de Gemini Parser

4. app/services/intent_service.py - _handle_conversation()
   ‚îî‚îÄ REGISTRA en memory_project_service despu√©s de generar

5. app/services/intent_service.py - _handle_display_content()
   ‚îî‚îÄ CONSULTA memory_project_service antes de Scene Graph

VERIFICACI√ìN:
-------------
Para cada modelo, verificar que el prompt incluya:

```
ACTIVE MEMORY PROJECTS:
‚ö° [note] Nota ABA
  ‚ö†Ô∏è PENDING DISPLAY - User may reference this content
```

Si el modelo NO ve esto, el contexto no se est√° propagando correctamente.

MODIFICACI√ìN 1: app/ai/context.py
----------------------------------
UBICACI√ìN: L√≠nea ~319 (dentro de UnifiedContext dataclass)
ACCI√ìN: AGREGAR campo nuevo (NO modificar existentes)

```python
# EXISTING CODE (NO TOCAR):
conversation_context: Optional[ConversationContext] = None

# NEW CODE (AGREGAR DESPU√âS):
# Sprint 4.1.1: Memory bridge for generated content tracking
memory_bridge: Optional[MemoryBridge] = None
```

UBICACI√ìN: L√≠nea ~1 (imports al inicio del archivo)
ACCI√ìN: AGREGAR import

```python
# EXISTING IMPORTS (NO TOCAR)
from app.ai.intent.schemas import Intent
# ... otros imports ...

# NEW IMPORT (AGREGAR):
from app.ai.memory.schemas import MemoryBridge
from app.ai.memory.service import memory_project_service
```

UBICACI√ìN: L√≠nea ~450 (dentro de build_unified_context, despu√©s de conversation_context)
ACCI√ìN: AGREGAR inyecci√≥n de memory bridge

```python
# EXISTING CODE (NO TOCAR):
conv_context = conversation_context_service.get_context(str(user_id))

# NEW CODE (AGREGAR DESPU√âS):
# Sprint 4.1.1: Inject memory bridge for content-aware intent classification
memory_bridge = memory_project_service.get_memory_bridge(str(user_id))

# EXISTING CODE (NO TOCAR):
context = UnifiedContext(
    user=user_context,
    devices=devices_context,
    oauth_status=oauth_status,
    conversation_context=conv_context,
    # NEW: AGREGAR ESTE CAMPO
    memory_bridge=memory_bridge,
)
```

MODIFICACI√ìN 2: app/services/intent_service.py
-----------------------------------------------
UBICACI√ìN: L√≠nea ~1 (imports)
ACCI√ìN: AGREGAR import

```python
# EXISTING IMPORTS (NO TOCAR)
from app.services.conversation_context_service import conversation_context_service
# ... otros imports ...

# NEW IMPORTS (AGREGAR):
from app.ai.memory.service import memory_project_service
from app.ai.memory.schemas import ContentType
```

UBICACI√ìN: L√≠nea ~1551 (dentro de _handle_conversation, DESPU√âS de generar el mensaje)
ACCI√ìN: AGREGAR registro de contenido generado

```python
# EXISTING CODE (generar mensaje con LLM - NO TOCAR):
message = response.get("message", "")

# NEW CODE (AGREGAR DESPU√âS, ANTES de conversation_context_service.add_conversation_turn):
# Sprint 4.1.1: Register generated content for display
# Detect if this is content generation (vs simple conversation)
is_content_generation = any(
    keyword in original_text.lower()
    for keyword in ["crea", "genera", "escribe", "create", "generate", "write", "draft"]
)

if is_content_generation and len(message) > 100:  # Substantial content
    # Detect content type from request
    content_type = "note"  # default
    if any(word in original_text.lower() for word in ["email", "correo"]):
        content_type = "email"
    elif any(word in original_text.lower() for word in ["mensaje", "message"]):
        content_type = "message"
    elif any(word in original_text.lower() for word in ["documento", "document"]):
        content_type = "document"
    elif any(word in original_text.lower() for word in ["resumen", "summary"]):
        content_type = "summary"
    elif any(word in original_text.lower() for word in ["script", "guion"]):
        content_type = "script"
    elif any(word in original_text.lower() for word in ["template", "plantilla"]):
        content_type = "template"

    # Extract title from request (simple heuristic)
    display_title = None
    if "nota" in original_text.lower():
        # Extract words after "nota" as title
        parts = original_text.lower().split("nota")
        if len(parts) > 1:
            title_part = parts[1].strip().split()[0:3]  # First 3 words
            display_title = "Nota " + " ".join(title_part).title()

    # Register the generated content
    memory_project = memory_project_service.register_generated_content(
        user_id=str(user_id),
        content=message,
        content_type=content_type,
        source_request=original_text,
        display_title=display_title,
        ttl_seconds=300,
    )

    logger.info(f"Registered memory project: {memory_project.project_id}")

# EXISTING CODE (conversation history - NO TOCAR):
conversation_context_service.add_conversation_turn(
    user_id=str(user_id),
    user_message=original_text,
    assistant_response=message,
    intent_type="conversation",
)
```

UBICACI√ìN: L√≠nea ~1594 (dentro de _handle_display_content o crear si no existe)
ACCI√ìN: AGREGAR uso de memoria para display

NOTA: Si _handle_display_content NO existe, cr√©alo siguiendo este template.
Si ya existe, MODIFICA solo para agregar memoria al inicio.

```python
async def _handle_display_content(
    self,
    request_id: str,
    intent: Any,  # DisplayContentIntent o DeviceCommand
    user_id: UUID,
    devices: List[Device],
    start_time: float,
    db: Session,
) -> IntentResult:
    """
    Handle DISPLAY_CONTENT intent using Scene Graph.

    Sprint 4.1.1: Now checks memory_project_service first for generated content.
    """
    import time

    # Sprint 4.1.1: Check if there's generated content to display
    memory_bridge = memory_project_service.get_memory_bridge(str(user_id))

    if memory_bridge.has_recent_generated_content():
        latest = memory_bridge.get_latest_generated()

        logger.info(
            f"Displaying generated content from memory: "
            f"{latest.project_id} ({latest.content_type.value})"
        )

        # Use Scene Graph with text_block component to display
        from app.ai.scene.service import SceneService
        from app.ai.scene.schemas import (
            SceneGraph,
            LayoutSpec,
            LayoutIntent,
            LayoutEngine,
            SceneComponent,
            ComponentPosition,
            ComponentPriority,
            SceneMetadata,
        )

        # Create scene with text_block component
        scene = SceneGraph(
            scene_id=str(uuid.uuid4()),
            version="1.1",
            target_devices=[str(d.id) for d in devices if d.is_online][:1],
            layout=LayoutSpec(
                intent=LayoutIntent.FULLSCREEN,
                engine=LayoutEngine.FLEX,
                direction="column",
            ),
            components=[
                SceneComponent(
                    id="generated_content",
                    type="text_block",
                    priority=ComponentPriority.PRIMARY,
                    position=ComponentPosition(flex=1),
                    props={
                        "content": latest.content,
                        "title": latest.display_title or "Generated Content",
                        "alignment": "left",
                    },
                    data={
                        "is_placeholder": False,
                        "content": latest.content,
                    },
                )
            ],
            metadata=SceneMetadata(
                created_at=datetime.now().isoformat(),
                ttl_seconds=3600,
                user_request=intent.original_text if hasattr(intent, 'original_text') else "Display content",
                generated_by="memory_project_service",
            ),
        )

        # Mark as displayed
        memory_project_service.mark_as_displayed(latest.project_id, str(user_id))

        processing_time = (time.time() - start_time) * 1000

        return IntentResult(
            success=True,
            intent_type=IntentResultType.DISPLAY_CONTENT,
            confidence=0.95,
            action="display_scene",
            parameters={"scene_graph": scene.model_dump()},
            message=f"Displaying generated {latest.content_type.value}",
            processing_time_ms=processing_time,
            request_id=request_id,
        )

    # EXISTING CODE: Si no hay contenido generado, usar Scene Graph normal
    # (c√≥digo existente de _handle_display_content si existe, o default scene)
    # NO MODIFICAR ESTE C√ìDIGO
```

MODIFICACI√ìN 3A: app/ai/prompts/base_prompt.py (PREFERIDO - DRY)
-----------------------------------------------------------------
CR√çTICO: Esta es la forma M√ÅS EFICIENTE de inyectar memoria en TODOS los modelos.
base_prompt.py ya construye el contexto base para Router, Executor, Reasoner y Parser.

UBICACI√ìN: L√≠nea ~96 (al final de build_base_system_prompt, ANTES del return)
ACCI√ìN: AGREGAR secci√≥n de memoria activa

```python
# EXISTING CODE (NO TOCAR):
AVAILABLE ACTIONS:
  {actions_section}

# NEW CODE (AGREGAR ANTES DEL RETURN):
"""

    # Sprint 4.1.1: Inject active memory projects
    if hasattr(context, 'memory_bridge') and context.memory_bridge:
        memory_context = context.memory_bridge.to_context_string()
        if memory_context:
            prompt += f"\n\n{memory_context}"

    return prompt

# EXISTING CODE (el return ya est√° arriba)
```

EXPLICACI√ìN:
------------
Al agregar memoria en build_base_system_prompt(), AUTOM√ÅTICAMENTE se inyecta en:
- build_router_prompt() (l√≠nea ~122) - usa build_base_system_prompt()
- build_executor_prompt() (l√≠nea ~XXX) - usa build_base_system_prompt()
- build_reasoner_prompt() (l√≠nea ~XXX) - usa build_base_system_prompt()
- build_intent_prompt() (l√≠nea ~XXX) - usa build_base_system_prompt()

Esto es DRY - una sola modificaci√≥n beneficia a TODOS los modelos.

MODIFICACI√ìN 3B: app/ai/router/orchestrator.py (ALTERNATIVO)
-------------------------------------------------------------
Si base_prompt.py NO se usa en orchestrator, agregar aqu√≠ tambi√©n:

UBICACI√ìN: L√≠nea ~164 (dentro de analyze_request, donde se inyecta context)
ACCI√ìN: AGREGAR inyecci√≥n de memory context en prompts

```python
# EXISTING CODE (NO TOCAR):
if context:
    context_str = f"\n\nContext:\n{json.dumps(context, indent=2)}"

# NEW CODE (AGREGAR DESPU√âS):
# Sprint 4.1.1: Inject memory bridge context for ALL models
# This ensures Gemini (router), Gemini (parser), GPT, and Claude
# ALL see active memory projects, not just on intent changes
if hasattr(context, 'memory_bridge') and context.memory_bridge:
    memory_context = context.memory_bridge.to_context_string()
    if memory_context:
        context_str += f"\n\n{memory_context}"
        logger.debug(f"Injected memory context: {memory_context}")
```

VERIFICACI√ìN DE TODOS LOS PROMPTS:
-----------------------------------

La carpeta app/ai/prompts/ tiene 9 archivos. Verificar d√≥nde se usa UnifiedContext:

‚úÖ base_prompt.py (CR√çTICO - MODIFICAR)
   - build_base_system_prompt(context) ‚Üê AGREGAR memoria aqu√≠
   - Este se usa en todos los otros prompts
   - Una modificaci√≥n beneficia a TODOS

‚úÖ router_prompts.py
   - Usa base_prompt.py ‚Üí heredar√° memoria autom√°ticamente
   - NO modificar (DRY)

‚úÖ execution_prompts.py
   - Usa base_prompt.py ‚Üí heredar√° memoria autom√°ticamente
   - NO modificar (DRY)

‚úÖ intent_prompts.py
   - MODIFICAR reglas (agregar MEMORY-AWARE CLASSIFICATION)
   - NO modificar build functions (ya heredan de base_prompt.py)

‚ö†Ô∏è assistant_prompts.py
   - VERIFICAR si usa UnifiedContext
   - Si usa base_prompt.py ‚Üí heredar√° autom√°ticamente
   - Si NO, agregar inyecci√≥n manual

‚ö†Ô∏è scene_prompts.py
   - VERIFICAR si usa UnifiedContext
   - Probablemente NO necesita memoria (solo layouts)

‚ö†Ô∏è doc_prompts.py
   - VERIFICAR si usa UnifiedContext
   - Probablemente necesita memoria (para distinguir docs generados vs Google Docs)

‚ö†Ô∏è calendar_search_prompts.py
   - VERIFICAR si usa UnifiedContext
   - Probablemente NO necesita memoria (solo b√∫squeda de eventos)

REGLA GENERAL:
--------------
Si un prompt usa build_base_system_prompt() ‚Üí heredar√° memoria autom√°ticamente
Si un prompt NO usa base_prompt.py ‚Üí agregar inyecci√≥n manual

VERIFICACI√ìN DE app/ai/intent/parser.py:
-----------------------------------------

UBICACI√ìN: Buscar donde se llama parse() o parse_intent()
ACCI√ìN: VERIFICAR que recibe context con memory_bridge

```python
# DEBE SER AS√ç (verificar):
intent = await self.parser.parse(
    text=original_text,
    context=unified_context,  # ‚Üê DEBE incluir memory_bridge
)

# NO as√≠ (sin contexto):
intent = await self.parser.parse(text=original_text)  # ‚ùå INCORRECTO
```

UBICACI√ìN: Dentro de IntentParser.parse() donde construye el prompt
ACCI√ìN: VERIFICAR si usa build_base_system_prompt() o build_intent_prompt()

Si USA base_prompt:
```python
# Ya heredar√° memoria, NO agregar nada
from app.ai.prompts.base_prompt import build_intent_prompt
prompt = build_intent_prompt(context, user_text)
# ‚Üë Ya incluye memoria autom√°ticamente
```

Si NO USA base_prompt:
```python
# AGREGAR inyecci√≥n manual:
# Sprint 4.1.1: Inject memory bridge for intent classification
if hasattr(context, 'memory_bridge') and context.memory_bridge:
    memory_context_str = context.memory_bridge.to_context_string()
    if memory_context_str:
        prompt += f"\n\n{memory_context_str}"
        logger.debug("Intent parser received memory context")
```

MODIFICACI√ìN 4: app/ai/prompts/intent_prompts.py
-------------------------------------------------
UBICACI√ìN: L√≠nea ~165 (despu√©s de reglas DOC_QUERY vs CONVERSATION)
ACCI√ìN: AGREGAR reglas memory-aware

```python
# EXISTING RULES (NO TOCAR)
# ... reglas de DOC_QUERY ...

# NEW RULES (AGREGAR):
"""
MEMORY-AWARE INTENT CLASSIFICATION:
------------------------------------

When ACTIVE MEMORY PROJECTS are present in context:

1. If user says "muestra la nota" AND there's [generated] note project:
   ‚Üí Intent: DISPLAY_CONTENT (not DOC_QUERY)
   ‚Üí Reason: User is referencing recently generated content

2. If user says "show the email" AND there's [generated] email project:
   ‚Üí Intent: DISPLAY_CONTENT (not DOC_QUERY)
   ‚Üí Reason: User wants to display AI-generated content

3. If user says "ponlo en la pantalla" with active memory project:
   ‚Üí Intent: DISPLAY_CONTENT
   ‚Üí Reason: "it" refers to generated content

4. DOC_QUERY requires EXTERNAL document reference:
   - "muestra el documento de Google Drive" ‚Üí DOC_QUERY (external)
   - "abre mi documento de ayer" ‚Üí DOC_QUERY (external)
   - Must explicitly reference Google Docs, Drive, or external storage

PRIORITY: Memory projects take precedence over external document search.
"""
```

================================================================================
                              TESTING STRATEGY
================================================================================

TESTS A CREAR (siguiendo patr√≥n existente):
--------------------------------------------

1. tests/ai/memory/test_schemas.py
   - Test MemoryProject creation
   - Test ContentStatus and ContentType enums
   - Test MemoryBridge.to_context_string()
   - Test MemoryProject.is_expired()

2. tests/ai/memory/test_service.py
   - Test register_generated_content()
   - Test get_memory_bridge()
   - Test TTL cleanup (_cleanup_expired)
   - Test mark_as_displayed()

3. tests/services/test_intent_service_memory.py
   - Test _handle_conversation() registers content
   - Test _handle_display_content() uses memory
   - Test flow: generate ‚Üí display

4. tests/ai/test_context_memory_bridge.py
   - Test build_unified_context() includes memory_bridge
   - Test memory context injection in prompts

VALIDACI√ìN:
-----------
- Todos los 679 tests existentes deben seguir pasando
- Los nuevos tests deben cubrir el flujo completo
- NO romper funcionalidad existente

================================================================================
                           ORDEN DE IMPLEMENTACI√ìN
================================================================================

FASE 1: Crear m√≥dulo base (sin integraci√≥n)
--------------------------------------------
1. Crear app/ai/memory/__init__.py
2. Crear app/ai/memory/schemas.py
3. Crear app/ai/memory/service.py
4. Crear tests/ai/memory/test_schemas.py
5. Crear tests/ai/memory/test_service.py
6. Verificar: Tests del m√≥dulo aislado pasan

FASE 2: Integrar con UnifiedContext
------------------------------------
7. Modificar app/ai/context.py (agregar campo memory_bridge)
8. Modificar app/ai/context.py (inyectar en build_unified_context)
9. Crear tests/ai/test_context_memory_bridge.py
10. Verificar: Tests de contexto pasan

FASE 3: Integrar con IntentService
-----------------------------------
11. Modificar app/services/intent_service.py (_handle_conversation)
12. Modificar app/services/intent_service.py (_handle_display_content)
13. Crear tests/services/test_intent_service_memory.py
14. Verificar: Tests de integraci√≥n pasan

FASE 4: Integrar con prompts
-----------------------------
15. Modificar app/ai/router/orchestrator.py (inyectar memoria en prompts)
16. Modificar app/ai/prompts/intent_prompts.py (reglas memory-aware)
17. Verificar: Intent classification funciona correctamente

FASE 5: Validaci√≥n final
-------------------------
18. Ejecutar TODOS los 679 tests existentes
19. Verificar que ninguno se rompi√≥
20. Ejecutar prueba end-to-end:
    - "Crea una nota ABA" ‚Üí genera y registra
    - "muestra la nota en la pantalla" ‚Üí detecta memoria y muestra

================================================================================
                              RESTRICCIONES
================================================================================

CRITICAL - NO HACER:
--------------------
‚ùå NO modificar conversation_context_service (ya funciona, es para otro prop√≥sito)
‚ùå NO modificar pending_event_service (es para eventos, no para contenido generado)
‚ùå NO duplicar llamadas a LLMs (usar providers existentes)
‚ùå NO modificar Scene Graph service (ya funciona, solo usar)
‚ùå NO romper tests existentes (679 tests deben pasar)
‚ùå NO crear nuevos endpoints REST (esto es backend-only)
‚ùå NO modificar WebSocket (no es necesario para esta feature)

MUST DO:
--------
‚úÖ Seguir patr√≥n TTL de pending_event_service (in-memory, 300s)
‚úÖ Seguir patr√≥n singleton de conversation_context_service
‚úÖ Usar Scene Graph existente para display (text_block component)
‚úÖ Inyectar contexto en UnifiedContext (no crear sistema paralelo)
‚úÖ Agregar logging en cada operaci√≥n importante
‚úÖ Escribir tests para cada componente nuevo
‚úÖ Preservar funcionalidad existente al 100%

================================================================================
                                 CHECKLIST
================================================================================

Antes de marcar como completo, verificar:

M√ìDULO NUEVO:
‚ñ° M√≥dulo app/ai/memory/ creado con __init__.py, schemas.py y service.py
‚ñ° MemoryProject, ContentStatus, ContentType definidos correctamente
‚ñ° MemoryBridge con to_context_string() implementado
‚ñ° memory_project_service singleton creado

INTEGRACI√ìN CON UNIFIED CONTEXT:
‚ñ° UnifiedContext tiene campo memory_bridge
‚ñ° build_unified_context() inyecta memory bridge correctamente
‚ñ° memory_bridge se crea ANTES de llamar a cualquier modelo

INTEGRACI√ìN CON PROMPTS (CR√çTICO):
‚ñ° base_prompt.py inyecta memoria en build_base_system_prompt()
‚ñ° VERIFICAR que router_prompts.py usa base_prompt.py
‚ñ° VERIFICAR que execution_prompts.py usa base_prompt.py
‚ñ° VERIFICAR que intent_prompts.py usa base_prompt.py
‚ñ° intent_prompts.py tiene reglas memory-aware AGREGADAS (no reemplazadas)
‚ñ° assistant_prompts.py verificado (usa base_prompt o inyecci√≥n manual)
‚ñ° doc_prompts.py verificado (si usa docs generados, necesita memoria)

INTEGRACI√ìN CON INTENT SERVICE:
‚ñ° _handle_conversation() registra contenido generado
‚ñ° Detecci√≥n de content_type funciona (note, email, message, etc.)
‚ñ° Extracci√≥n de display_title funciona
‚ñ° _handle_display_content() existe o fue creado
‚ñ° _handle_display_content() consulta memoria PRIMERO
‚ñ° _handle_display_content() usa Scene Graph con text_block
‚ñ° _handle_display_content() marca como DISPLAYED despu√©s de mostrar

PROPAGACI√ìN ENTRE MODELOS:
‚ñ° AI Router (Gemini Flash) ve memory_bridge
‚ñ° Intent Parser (Gemini Flash) ve memory_bridge
‚ñ° GPT-5.2 (Conversation) ve memory_bridge
‚ñ° Claude Opus 4.5 (Reasoning) ve memory_bridge
‚ñ° Logging confirma: "Injected memory context" en cada modelo

TESTS:
‚ñ° tests/ai/memory/test_schemas.py creado y pasa
‚ñ° tests/ai/memory/test_service.py creado y pasa
‚ñ° tests/services/test_intent_service_memory.py creado y pasa
‚ñ° tests/ai/test_context_memory_bridge.py creado y pasa
‚ñ° Todos los 679 tests existentes siguen pasando

FLUJO END-TO-END:
‚ñ° Request #1: "crea una nota ABA"
  ‚ñ° Sistema: genera contenido con LLM
  ‚ñ° Sistema: registra en memory_project_service
  ‚ñ° Sistema: guarda en conversation_history
  ‚ñ° Response: contenido generado

‚ñ° Request #2: "muestra la nota en la pantalla"
  ‚ñ° build_unified_context() incluye memory_bridge con proyecto
  ‚ñ° AI Router ve: "ACTIVE MEMORY PROJECTS: ‚ö° [note] Nota ABA"
  ‚ñ° Intent Parser ve: "ACTIVE MEMORY PROJECTS: ‚ö° [note] Nota ABA"
  ‚ñ° Intent Parser clasifica: DISPLAY_CONTENT (NO DOC_QUERY)
  ‚ñ° _handle_display_content() encuentra proyecto en memoria
  ‚ñ° Scene Graph creado con text_block component
  ‚ñ° Proyecto marcado como DISPLAYED
  ‚ñ° Response: SceneGraph con contenido embebido

PRINCIPIOS DRY/SOLID:
‚ñ° No hay c√≥digo duplicado (una modificaci√≥n en base_prompt beneficia a todos)
‚ñ° No se modific√≥ conversation_context_service (mantiene su responsabilidad)
‚ñ° No se modific√≥ pending_event_service (mantiene su responsabilidad)
‚ñ° No se duplicaron llamadas a LLMs (usa providers existentes)
‚ñ° No se duplic√≥ l√≥gica de Scene Graph (usa SceneService existente)
‚ñ° Cada clase tiene una √∫nica responsabilidad

LOGGING Y DEBUG:
‚ñ° memory_project_service logea registros y cleanup
‚ñ° build_unified_context() logea creaci√≥n de memory_bridge
‚ñ° base_prompt.py logea inyecci√≥n de memoria
‚ñ° _handle_conversation() logea registro de proyectos
‚ñ° _handle_display_content() logea uso de memoria

RESTRICCIONES:
‚ñ° NO se modific√≥ c√≥digo que funciona
‚ñ° NO se crearon endpoints REST nuevos
‚ñ° NO se modific√≥ WebSocket
‚ñ° NO se rompi√≥ funcionalidad existente

================================================================================
                                    FIN
================================================================================

Este prompt proporciona TODO lo necesario para implementar el m√≥dulo Memory
Projects correctamente, siguiendo principios DRY y SOLID, sin romper nada
existente, y resolviendo el problema de los logs de forma limpia y escalable.
